# MDP-формализация среды

Мы моделируем задачу как марковский процесс принятия решений (MDP). Он состоит из следующих элементов: состояния, действия, переходы, награда, коэффициент дисконтирования и начальное состояние.

## Пространство состояний
Состояние в любой момент времени включает:
- Загаданное число (x*),
- Последняя попытка угадывания (last_guess),
- Подсказка (NONE, HIGHER, LOWER, CORRECT),
- Текущий шаг (t).

Агент видит только последнюю попытку и подсказку. Все возможные состояния — это комбинации загаданного числа (от 1 до N), попыток (от 0 до N), подсказок и шагов.

## Пространство действий
Действия — это числа от 1 до N, которые агент может угадывать.

## Переходы
Когда агент выбирает действие (a):
- Последняя попытка обновляется до выбранного числа,
- Подсказка становится:
  - CORRECT, если угадал (a = x*),
  - HIGHER, если число меньше загаданного (a < x*),
  - LOWER, если число больше загаданного (a > x*),
- Шаг увеличивается на 1.

Эпизод заканчивается, если число угадано или достигнуто максимальное количество шагов.

## Начальное состояние
- Загаданное число (x*) выбирается случайно от 1 до N,
- Последняя попытка = 0,
- Подсказка = NONE,
- Шаг = 0.

## Награда
- Если агент угадал (a = x*), награда = 1,
- Если не угадал, награда = 0 (можно добавить штраф, например, -0.1).

## Дисконтирование
Коэффициент дисконтирования (γ) обычно равен 0.95.

# Политика и обучение

## Выбор действий
Агент действует по ε-жадной стратегии:
- С вероятностью (1 - ε) выбирает лучшее действие по Q-функции,
- С вероятностью ε выбирает случайное действие.

## Обновление Q-функции
Q-функция обновляется так: берется старая Q-оценка, добавляется награда и лучшая оценка следующего состояния с учетом шага обучения (α) и дисконтирования (γ).

## Гиперпараметры
- Шаг обучения (α) = 0.1,
- Дисконтирование (γ) = 0.95,
- Вероятность случайного выбора (ε) = 0.1 (можно уменьшать со временем).
